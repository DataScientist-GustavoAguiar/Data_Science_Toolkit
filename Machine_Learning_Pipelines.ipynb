{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine_Learning_Pipelines.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afDMZrmfUIZ5"
      },
      "source": [
        "## Machine Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGDoGaU-AESt"
      },
      "source": [
        "### Machine Learning pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxtGe20zX8pC"
      },
      "source": [
        "Advantages of Using Pipeline Part 1:\n",
        "1. Simplicity and Convencience\n",
        "* Automates repetitive steps - Chaining all of your steps into one estimator allows you to fit and predict on all steps of your sequence automatically with one call. It handles smaller steps for you, so you can focus on implementing higher level changes swiftly and efficiently.\n",
        "* Easily understandable workflow - Not only does this make your code more concise, it also makes your workflow much easier to understand and modify. Without Pipeline, your model can easily turn into messy spaghetti code from all the adjustments and experimentation required to improve your model.\n",
        "* Reduces mental workload - Because Pipeline automates the intermediate actions required to execute each step, it reduces the mental burden of having to keep track of all your data transformations. Using Pipeline may require some extra work at the beginning of your modeling process, but it prevents a lot of headaches later on.\n",
        "\n",
        "Advantages of Using Pipeline Part 2:\n",
        "2. Optimizing Entire Workflow\n",
        "GRID SEARCH: Method that automates the process of testing different hyper parameters to optimize a model.\n",
        "By running grid search on your pipeline, you're able to optimize your entire workflow, including data transformation and modeling steps. This accounts for any interactions among the steps that may affect the final metrics.\n",
        "Without grid search, tuning these parameters can be painfully slow, incomplete, and messy.\n",
        "3. Preventing Data leakage\n",
        "Using Pipeline, all transformations for data preparation and feature extractions occur within each fold of the cross validation process.\n",
        "This prevents common mistakes where you’d allow your training process to be influenced by your test data - for example, if you used the entire training dataset to normalize or extract features from your data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prQwLX0OLX7c"
      },
      "source": [
        "\n",
        "# Machine Learning Workflow (Incomplete)\n",
        "def main():\n",
        "    X, y = load_data()\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "    # build pipeline\n",
        "     pipeline = Pipeline([\n",
        "        ('vect', CountVectorizer(tokenizer=tokenize)),\n",
        "        ('tfidf', TfidfTransformer()),\n",
        "        ('clf', RandomForestClassifier())\n",
        "    ])\n",
        "   \n",
        "    # train classifier\n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # predict on test data\n",
        "    y_pred = pipeline.predict(X_test)\n",
        "\n",
        "    # display results\n",
        "    display_results(y_test, y_pred)\n",
        "\n",
        "''' Pipelines And Feature Unions ----------------------------------------------\n",
        "\n",
        "FEATURE UNION: Feature union is a class in scikit-learn’s Pipeline module that allows us to perform steps in parallel and take the union of their results for the next step.\n",
        "A pipeline performs a list of steps in a linear sequence, while a feature union performs a list of steps in parallel and then combines their results.\n",
        "In more complex workflows, multiple feature unions are often used within pipelines, and multiple pipelines are used within feature unions. '''\n",
        "\n",
        "X = df['text'].values\n",
        "y = df['label'].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "# Pipeline -> Sequence\n",
        "pipeline = Pipeline([\n",
        "    # Feature Union -> Parallel\n",
        "    ('features', FeatureUnion([\n",
        "                               \n",
        "        # Pipeline -> Sequence\n",
        "        ('nlp_pipeline', Pipeline([\n",
        "            ('vect', CountVectorizer()\n",
        "            ('tfidf', TfidfTransformer())\n",
        "        ])),\n",
        "\n",
        "        ('txt_len', TextLengthExtractor())\n",
        "    ])),\n",
        "\n",
        "    ('clf', RandomForestClassifier())\n",
        "])\n",
        "\n",
        "# train classifier\n",
        "pipeline.fit(Xtrain)\n",
        "\n",
        "# predict on test data\n",
        "predicted = pipeline.predict(Xtest)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "''' Creating a custom transformer --------------------------------------------''' \n",
        "\n",
        "''' Remember, all estimators have a fit method, and since this is a transformer,\n",
        " it also has a transform method.\n",
        "\n",
        "FIT METHOD: This takes in a 2d array X for the feature data and a 1d array y for \n",
        "the target labels. Inside the fit method, we simply return self. This allows us \n",
        "to chain methods together, since the result on calling fit on the transformer is \n",
        "still the transformer object. This method is required to be compatible with \n",
        "scikit-learn.\n",
        "\n",
        "TRANSFORM METHOD: The transform function is where we include the code that well, \n",
        "transforms the data. In this case, we return the data in X multiplied by 10. This\n",
        " transform method also takes a 2d array X.'''\n",
        "\n",
        "# Example 1 ------------------------------------------------------------------\n",
        "# Creating\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class TenMultiplier(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return X * 10\n",
        "\n",
        "# Applying\n",
        "multiplier = TenMultiplier()\n",
        "\n",
        "X = np.array([6, 3, 7, 4, 7])\n",
        "multiplier.transform(X)\n",
        "\n",
        "\n",
        "# Example 2 ------------------------------------------------------------------\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "# Creating\n",
        "class CaseNormalizer(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return pd.Series(X).apply(lambda x: x.lower()).values\n",
        "\n",
        "# Applying\n",
        "case_normalizer = CaseNormalizer()\n",
        "\n",
        "X = np.array(['Implementing', 'a', 'Custom', 'Transformer', 'from', 'SCIKIT-LEARN'])\n",
        "case_normalizer.transform(X)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "''' Partial Example -------------------------------------------------------''' \n",
        "import nltk\n",
        "nltk.download(['punkt', 'wordnet', 'averaged_perceptron_tagger'])\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "\n",
        "url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
        "\n",
        "\n",
        "class StartingVerbExtractor(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    def starting_verb(self, text):\n",
        "        sentence_list = nltk.sent_tokenize(text)\n",
        "        for sentence in sentence_list:\n",
        "            pos_tags = nltk.pos_tag(tokenize(sentence))\n",
        "            first_word, first_tag = pos_tags[0]\n",
        "            if first_tag in ['VB', 'VBP'] or first_word == 'RT':\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_tagged = pd.Series(X).apply(self.starting_verb)\n",
        "        return pd.DataFrame(X_tagged)\n",
        "\n",
        "\n",
        "def load_data():\n",
        "    df = pd.read_csv('corporate_messaging.csv', encoding='latin-1')\n",
        "    df = df[(df[\"category:confidence\"] == 1) & (df['category'] != 'Exclude')]\n",
        "    X = df.text.values\n",
        "    y = df.category.values\n",
        "    return X, y\n",
        "\n",
        "\n",
        "def tokenize(text):\n",
        "    detected_urls = re.findall(url_regex, text)\n",
        "    for url in detected_urls:\n",
        "        text = text.replace(url, \"urlplaceholder\")\n",
        "\n",
        "    tokens = word_tokenize(text)\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    clean_tokens = []\n",
        "    for tok in tokens:\n",
        "        clean_tok = lemmatizer.lemmatize(tok).lower().strip()\n",
        "        clean_tokens.append(clean_tok)\n",
        "\n",
        "    return clean_tokens\n",
        "\n",
        "\n",
        "def model_pipeline():\n",
        "    pipeline = Pipeline([\n",
        "        ('features', FeatureUnion([\n",
        "\n",
        "            ('text_pipeline', Pipeline([\n",
        "                ('vect', CountVectorizer(tokenizer=tokenize)),\n",
        "                ('tfidf', TfidfTransformer())\n",
        "            ])),\n",
        "\n",
        "            ('starting_verb', StartingVerbExtractor())\n",
        "        ])),\n",
        "\n",
        "        ('clf', RandomForestClassifier())\n",
        "    ])\n",
        "\n",
        "    return pipeline\n",
        "\n",
        "\n",
        "def display_results(y_test, y_pred):\n",
        "    labels = np.unique(y_pred)\n",
        "    confusion_mat = confusion_matrix(y_test, y_pred, labels=labels)\n",
        "    accuracy = (y_pred == y_test).mean()\n",
        "\n",
        "    print(\"Labels:\", labels)\n",
        "    print(\"Confusion Matrix:\\n\", confusion_mat)\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "\n",
        "\n",
        "def main():\n",
        "    X, y = load_data()\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "    model = model_pipeline()\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    display_results(y_test, y_pred)\n",
        "\n",
        "main()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "''' Using Pipeline with GridSearchCV -------------------------------------- '''\n",
        "\n",
        "# It is important to protect our ML workflow. Without the pipeline is possible to have data leakages.\n",
        "\n",
        "''' This may seem okay at first, but if you standardize your whole training dataset,\n",
        "and then use cross validation in grid search to evaluate your model, you've got\n",
        "data leakage. Let me explain. Grid search uses cross validation to score your\n",
        "model, meaning it splits your training data into folds of train and validation\n",
        "sets, trains your model on the train set, and scores it on the validation set,\n",
        "and does this multiple times.\n",
        "\n",
        "However, each time, or fold, that this happens, the model already has knowledge\n",
        "of the validation set because all the data was rescaled based on the distribution\n",
        "of the whole training dataset. Important factors like the mean and standard \n",
        "deviation are influenced by the whole dataset. This means the model perform \n",
        "better than it really should on unseen data, since information about the \n",
        "validation set is always baked into the rescaled values of your train dataset.\n",
        "\n",
        "The way to fix this, would be to make sure you run standard scaler only on the \n",
        "training set, and not the validation set within each fold of cross validation. \n",
        "Pipelines allow you to do just this.'''\n",
        "\n",
        "# since the rescaling is included as part of the pipeline, the standardization \n",
        "# doesn't happen until we run grid search. Meaning in each fold of cross validation,\n",
        "# the rescaling is done only on the data that the model is trained on, preventing \n",
        "# leakage from the validation set.\n",
        "# pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('clf', SVC())\n",
        "])\n",
        "\n",
        "parameters = {\n",
        "    'scaler__with_mean': [True, False]\n",
        "    'clf__kernel': ['linear', 'rbf'],\n",
        "    'clf__C':[1, 10]\n",
        "}\n",
        "\n",
        "cv = GridSearchCV(pipeline, param_grid=parameters)\n",
        "\n",
        "cv.fit(X_train, y_train)\n",
        "y_pred = cv.predict(X_test)\n",
        "\n",
        "\n",
        "\n",
        "''' Complete Example -------------------------------------------------------''' \n",
        "\n",
        "import nltk\n",
        "nltk.download(['punkt', 'wordnet', 'averaged_perceptron_tagger'])\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "\n",
        "url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
        "\n",
        "\n",
        "class StartingVerbExtractor(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    def starting_verb(self, text):\n",
        "        sentence_list = nltk.sent_tokenize(text)\n",
        "        for sentence in sentence_list:\n",
        "            pos_tags = nltk.pos_tag(tokenize(sentence))\n",
        "            first_word, first_tag = pos_tags[0]\n",
        "            if first_tag in ['VB', 'VBP'] or first_word == 'RT':\n",
        "                return 1\n",
        "        return 0\n",
        "\n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_tagged = pd.Series(X).apply(self.starting_verb)\n",
        "        return pd.DataFrame(X_tagged)\n",
        "\n",
        "\n",
        "def load_data():\n",
        "    df = pd.read_csv('corporate_messaging.csv', encoding='latin-1')\n",
        "    df = df[(df[\"category:confidence\"] == 1) & (df['category'] != 'Exclude')]\n",
        "    X = df.text.values\n",
        "    y = df.category.values\n",
        "    return X, y\n",
        "\n",
        "\n",
        "def tokenize(text):\n",
        "    detected_urls = re.findall(url_regex, text)\n",
        "    for url in detected_urls:\n",
        "        text = text.replace(url, \"urlplaceholder\")\n",
        "\n",
        "    tokens = word_tokenize(text)\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    clean_tokens = []\n",
        "    for tok in tokens:\n",
        "        clean_tok = lemmatizer.lemmatize(tok).lower().strip()\n",
        "        clean_tokens.append(clean_tok)\n",
        "\n",
        "    return clean_tokens\n",
        "\n",
        "\n",
        "def build_model():\n",
        "    pipeline = Pipeline([\n",
        "        ('features', FeatureUnion([\n",
        "\n",
        "            ('text_pipeline', Pipeline([\n",
        "                ('vect', CountVectorizer(tokenizer=tokenize)),\n",
        "                ('tfidf', TfidfTransformer())\n",
        "            ])),\n",
        "\n",
        "            ('starting_verb', StartingVerbExtractor())\n",
        "        ])),\n",
        "\n",
        "        ('clf', RandomForestClassifier())\n",
        "    ])\n",
        "\n",
        "    parameters = {\n",
        "        'features__text_pipeline__vect__ngram_range': ((1, 1), (1, 2)),\n",
        "        'features__text_pipeline__vect__max_df': (0.5, 0.75, 1.0),\n",
        "        'features__text_pipeline__vect__max_features': (None, 5000, 10000),\n",
        "        'features__text_pipeline__tfidf__use_idf': (True, False),\n",
        "        'clf__n_estimators': [50, 100, 200],\n",
        "        'clf__min_samples_split': [2, 3, 4],\n",
        "        'features__transformer_weights': (\n",
        "            {'text_pipeline': 1, 'starting_verb': 0.5},\n",
        "            {'text_pipeline': 0.5, 'starting_verb': 1},\n",
        "            {'text_pipeline': 0.8, 'starting_verb': 1},\n",
        "        )\n",
        "    }\n",
        "\n",
        "    cv = GridSearchCV(pipeline, param_grid=parameters)\n",
        "\n",
        "    return cv\n",
        "\n",
        "\n",
        "def display_results(cv, y_test, y_pred):\n",
        "    labels = np.unique(y_pred)\n",
        "    confusion_mat = confusion_matrix(y_test, y_pred, labels=labels)\n",
        "    accuracy = (y_pred == y_test).mean()\n",
        "\n",
        "    print(\"Labels:\", labels)\n",
        "    print(\"Confusion Matrix:\\n\", confusion_mat)\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "    print(\"\\nBest Parameters:\", cv.best_params_)\n",
        "\n",
        "\n",
        "def main():\n",
        "    X, y = load_data()\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "    model = build_model()\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    display_results(model, y_test, y_pred)\n",
        "\n",
        "main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20LpVBAjLxvS"
      },
      "source": [
        "#### Machine Learning Workflows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_QeWe5NL0yt"
      },
      "source": [
        "\n",
        "''' Example of a ML workflow with textual data '''\n",
        "\n",
        "import nltk\n",
        "nltk.download(['punkt', 'wordnet'])\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "\n",
        "url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
        "\n",
        "# Loading the data\n",
        "def load_data():\n",
        "    df = pd.read_csv('corporate_messaging.csv', encoding='latin-1')\n",
        "    df = df[(df[\"category:confidence\"] == 1) & (df['category'] != 'Exclude')]\n",
        "    X = df.text.values\n",
        "    y = df.category.values\n",
        "    return X, y\n",
        "\n",
        "# Preprocessing textual data\n",
        "def tokenize(text):\n",
        "    detected_urls = re.findall(url_regex, text)\n",
        "    for url in detected_urls:\n",
        "        text = text.replace(url, \"urlplaceholder\")\n",
        "\n",
        "    tokens = word_tokenize(text)\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    clean_tokens = []\n",
        "    for tok in tokens:\n",
        "        clean_tok = lemmatizer.lemmatize(tok).lower().strip()\n",
        "        clean_tokens.append(clean_tok)\n",
        "\n",
        "    return clean_tokens\n",
        "\n",
        "# Display\n",
        "def display_results(y_test, y_pred):\n",
        "    labels = np.unique(y_pred)\n",
        "    confusion_mat = confusion_matrix(y_test, y_pred, labels=labels)\n",
        "    accuracy = (y_pred == y_test).mean()\n",
        "\n",
        "    print(\"Labels:\", labels)\n",
        "    print(\"Confusion Matrix:\\n\", confusion_mat)\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Running the workflow\n",
        "def main():\n",
        "    # Load data\n",
        "    X, y = load_data()\n",
        "\n",
        "    # Perform train test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "    \n",
        "    # Instantiate transformers and classifier\n",
        "    vect = CountVectorizer(tokenizer=tokenize)\n",
        "    tfidf = TfidfTransformer()\n",
        "    clf = RandomForestClassifier()\n",
        "\n",
        "    # Fit and/or transform each to the training data\n",
        "    X_train_counts = vect.fit_transform(X_train)\n",
        "    X_train_tfidf = tfidf.fit_transform(X_train_counts)\n",
        "\n",
        "    # Fit or train the classifier\n",
        "    clf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "    # Transform test data\n",
        "    X_test_counts = vect.transform(X_test)\n",
        "    X_test_tfidf = tfidf.transform(X_test_counts)\n",
        "\n",
        "    # Predict test labels\n",
        "    y_pred = clf.predict(X_test_tfidf)\n",
        "\n",
        "    # Display the results\n",
        "    display_results(y_test, y_pred)\n",
        "\n",
        "\n",
        "main()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}